# TMLC-Fellowship-Task-
This is my TMLC Fellowship task 
<a href="https://acehacker.com/microsoft/engage2022/">
	<img src="https://github.com/PriyankaKumari-2002/TMLC-Fellowship-Task-/blob/main/bert_encoder.png?raw=true width="450", height="130" alt="TLMC Fellowship 22"/>
</a>

## ğŸ“‹ Project Overview
This is my TLMC Fellowship Task for Ecommerce Text Classification to recognize whether the given description is related to Electronics (0), Households (1), Books (2), or Clothing & Accessories (3) products.
<br>
<br>

ğŸ‘©â€##ğŸ’»Algorithm Used: BERT (Bidirectional Encoder Representations from Transformers)

## BERT is based on the Transformer architecture

ABOUT BERT: Bidirectional Encoder Representations from Transformers is known as BERT. It is intended to jointly condition on both left and right context to pre-train deep bidirectional representations from unlabeled text. With just one additional output layer, the pre-trained BERT model can be improved to produce cutting-edge models for a variety of NLP tasks.


## ğŸ”– Reason behind using BERT:
In many natural language problems, BERT outperforms the state-of-the-art by assisting machines in learning excellent representations of text in relation to context.
## ğŸ”– Introduction to Bert :
	
As opposed to directional models, which read the text input sequentially (left-to-right or right-to-left), the Transformer encoder reads the entire sequence of words at once. Therefore it is considered bidirectional, though it would be more accurate to say that itâ€™s non-directional. This characteristic allows the model to learn the context of a word based on all of its surroundings (left and right of the word).
												    

## ğŸ’¡Note: In practice, the BERT implementation is slightly more elaborate and doesnâ€™t replace all of the 15% masked words.
	
